# Data Science Practicum 2020

This Data Science course has been taught at the Faculty of Science, Masaryk University, in the fall semester 2020/2021.  

I gave 12 lectures, each focused on one ML technique and dataset (typically from [kaggle.com](http://kaggle.com)). The emphasis has been on coding and practicing data science skills, rather than the theoretical background.

## Course Info

The course is now in the [IS Course Catalogue](https://is.muni.cz/), look for [M7DataSP](https://is.muni.cz/auth/course/sci/podzim2020/M7DATASP) Data Science Practicum (Praktikum z pokročilé datové vědy). 

The course is scheduled for **Mondays, 12:00-13:30**, and will be taught remotely through Google Meet/Hangouts. The first lecture will take place on **October 5**. To be invited to the classes, enroll to the course in [IS](https://is.muni.cz/). If you want to try a few first lectures without a formal enrollment, send me [an email](https://www.muni.cz/lide/244334-petr-simecek).

No special knowledge is expected but you should have at least one year of coding experience, either R or Python. I like diverse crowds; students from different faculties and specializations are encouraged to enroll (if still in doubt, let me know to be paired with a more experienced student). The course will be taught in English if at least two students will be interested, otherwise in Czech.

## Lectures

  1. Intro, linear regression (one neuron), neural networks (NN), TensorFlow (TF)
  1. Logistic regression, softmax, cross-entropy
  1. Image data, convolutional NN
  1. ImageNet, fine tuning, tranfer learning, data augmentation
  1. TenforFlowJS, GitHub Pages, backpropagation
  1. Natural language processing (NLP), text preprocessing, dense NN
  1. Embeddings, recurrent NN (LSTM, GRU)
  1. Text classification, transformers, NLP methods on genomic data
  1. Recommenders / Collaborative filtering, optimization
  1. Tabular data, batch normalization
  1. Trees, random forest, XGBoost, LightGBM, CatBoost
  1. ML models interpretation, hyper-parameters optimization, autoML

Recordings of the lectures can be found in [the teaching materials](https://is.muni.cz/auth/el/sci/podzim2020/M7DataSP/um/) in IS (you need to have MU GSuite to access the videos).

## Assignments

  1. Create a GitHub account and your first repository
  1. Classify penguins based on their size
  1. Fashion MNIST classification
  1. Image classification app
  1. Text generation
  1. Genomic seqs classification
  1. Ratings prediction
  1. Blue book for buldosers 

For the students' solution, see [Assignments.md](Assignments.md).

## Recommended books and blogs

  1. [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd edition)](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)  
  1. [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)
  1. [TensorFlow 2 in 30 days](https://github.com/lyhue1991/eat_tensorflow2_in_30_days)
  1. [Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD](https://github.com/fastai/fastbook)
  1. [RStudio AI Blog](https://blogs.rstudio.com/ai/)
  1. [The Missing Semester of Your CS Education](https://missing.csail.mit.edu/)


## Acknowledgement

This work would be impossible without tutorials provided by [TensorFlow](https://www.tensorflow.org/tutorials) and [RStudio](https://tensorflow.rstudio.com/tutorials/). I also get a lot  of inspiration from numerous Kagle notebooks and blogs all over the internet. Sometimes, in a time pressure before the lecture, I might have forgotten to properly link all my sources. If this is the case, I would be grateful if you correct my mistake, either by a pull request or sending me a message. Thank you. 
