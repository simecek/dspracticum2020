# Data Science Practicum 2020

This is a proposal for a Data Science course to be taught at the Faculty of Science, Masaryk University, in the fall semester 2020/2021.  

I expect to give 12 lectures, each focused on one dataset (typically from [kaggle.com](http://kaggle.com)) and one data science technique. The emphasis should be on coding and practicing data science skills, not the theoretical background.

## Course Info

The course is now in the [IS Course Catalogue](https://is.muni.cz/), look for [M7DataSP](https://is.muni.cz/auth/course/sci/podzim2020/M7DATASP) Data Science Practicum (Praktikum z pokročilé datové vědy). 

The course is scheduled for **Mondays, 12:00-13:30**, and will be taught remotely through Google Meet/Hangouts. The first lecture will take place on **October 5**. To be invited to the classes, enroll to the course in [IS](https://is.muni.cz/). If you want to try a few first lectures without a formal enrollment, send me [an email](https://www.muni.cz/lide/244334-petr-simecek).

No special knowledge is expected but you should have at least one year of coding experience, either R or Python. I like diverse crowds; students from different faculties and specializations are encouraged to enroll (if still in doubt, let me know to be paired with a more experienced student). The course will be taught in English if at least two students will be interested, otherwise in Czech.

## Lectures 

  1. **Linear regression** [[data]](https://www.kaggle.com/c/house-prices-advanced-regression-techniques), git and GitHub
  1. **Logistic regression** [[data]](https://www.kaggle.com/c/titanic), splitting data into train, validation and testing sets
  1. **Unsupervised methods** [[data]](https://www.kaggle.com/zynicide/wine-reviews), visualizations
  1. **Trees and forests** [[data]](https://www.kaggle.com/mlg-ulb/creditcardfraud)
  1. **XGBoost & friends** [[data]](https://www.kaggle.com/mlg-ulb/creditcardfraud)
  1. **Review**
  1. **TensorFlow, Keras, neural networks** 
  1. **Classification of images** [[data]](https://www.kaggle.com/zalando-research/fashionmnist)
  1. **Fine-tuning, transfer learning** [[data]](https://www.kaggle.com/zippyz/cats-and-dogs-breeds-classification-oxford-dataset)
  1. **Neural networks applied to natural language processing** [[data]](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)
  1. **Neural networks applied to tabular data** [[data]](https://www.kaggle.com/c/rossmann-store-sales)
  1. **Collaborative filtering** [[data]](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)

Recordings of the lectures can be found in [the teaching materials](https://is.muni.cz/auth/el/sci/podzim2020/M7DataSP/um/) in IS (you need to have MU GSuite to access the videos).

## Scoring and credits

50% homeworks (by group of 2-4 students), 50% final project (individual). To pass, you must achieve at least 60% points.

## Recommended books and blogs

  1. [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd edition)](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)  
  1. [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)
  1. [TensorFlow 2 in 30 days](https://github.com/lyhue1991/eat_tensorflow2_in_30_days)
  1. [Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD](https://github.com/fastai/fastbook)
  1. [RStudio AI Blog](https://blogs.rstudio.com/ai/)
  1. [The Missing Semester of Your CS Education](https://missing.csail.mit.edu/)


## Acknowledgement

This work would be impossible without tutorials provided by [TensorFlow](https://www.tensorflow.org/tutorials) and [RStudio](https://tensorflow.rstudio.com/tutorials/). I also get a lot  of inspiration from numerous Kagle notebooks and blogs all over the internet. Sometimes, in a time pressure before the lecture, I might have forgotten to properly link all my sources. If this is the case, I would be grateful if you correct my mistake, either by a pull request or sending me a message. Thank you. 