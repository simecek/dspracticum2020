# Lecture 11

**Date**: Dec 14, 2020

**Slides**: https://docs.google.com/presentation/d/1aByr-TaF5Ltvtjj9-qKR4mm7yxQ9nTlPfH2exalBzqY/edit?usp=sharing

* Classification / regression trees: sklearn.DecissionTreeClassifier, rpart
* Random forest (bagging): sklearn.ensemble.RandomForestRegressor, randomForest
* Gradient Boosted Trees (xgboost)
* Improved implementations: LightGBM, CatBoost

## Further reading

* Chapter 9 of J. Howard's and S. Gugger's [Fastbook](https://github.com/fastai/fastbook)

## Asignment 11

**Deadline**: January 4th, 10 am

Take the Kaggle dataset

https://www.kaggle.com/c/bluebook-for-bulldozers 

and try to train simple Random Forest or some kind of Gradient Boosted Tree model. Save the model and its predictions to GitHub repository.



